{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae26240",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.table import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, max, min, asc, desc\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DateType, DoubleType, ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "add0cf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"spark-s3\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "\n",
    "hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "hadoop_conf.set(\n",
    "    \"fs.s3a.aws.credentials.provider\",\n",
    "    \"com.amazonaws.auth.DefaultAWSCredentialsProviderChain\"\n",
    ")\n",
    "\n",
    "hadoop_conf.set(\"fs.s3a.path.style.access\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0846fa5-cc07-40b8-8b32-4249d733fc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_aws_path = 's3a://vd-airflow-docker-bucket/covid/Canada'\n",
    "\n",
    "curated_aws_path = 's3a://vd-airflow-docker-bucket/covid/Canada/curated'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "139dbd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema definition\n",
    "\n",
    "region_schema = StructType([\n",
    "    StructField('cities', ArrayType(StringType()), nullable = True),\n",
    "    StructField('iso', StringType(), nullable = True),\n",
    "    StructField('lat', StringType(), nullable = True),\n",
    "    StructField('long', StringType(), nullable = True),\n",
    "    StructField('name', StringType(), nullable = True),\n",
    "    StructField('province', StringType(), nullable = True)\n",
    "])\n",
    "\n",
    "raw_schema = StructType([\n",
    "    StructField('active', IntegerType(), nullable = True),\n",
    "    StructField('active_diff', IntegerType(), nullable = True),\n",
    "    StructField('confirmed', IntegerType(), nullable = True),\n",
    "    StructField('confirmed_diff', IntegerType(), nullable = True),\n",
    "    StructField('date', DateType(), nullable = True),\n",
    "    StructField('deaths', IntegerType(), nullable = True),\n",
    "    StructField('deaths_diff', IntegerType(), nullable = True),\n",
    "    StructField('fatality_rate', DoubleType(), nullable = True),\n",
    "    StructField('last_updated', IntegerType(), nullable = True),\n",
    "    StructField('recovered', IntegerType(), nullable = True),\n",
    "    StructField('recovered_diff', IntegerType(), nullable = True),\n",
    "    StructField('region', region_schema, nullable = True)\n",
    "])\n",
    "\n",
    "target_schema = StructType([\n",
    "    StructField('active', IntegerType(), nullable = True),\n",
    "    StructField('active_diff', IntegerType(), nullable = True),\n",
    "    StructField('confirmed', IntegerType(), nullable = True),\n",
    "    StructField('confirmed_diff', IntegerType(), nullable = True),\n",
    "    StructField('date', DateType(), nullable = True),\n",
    "    StructField('deaths', IntegerType(), nullable = True),\n",
    "    StructField('deaths_diff', IntegerType(), nullable = True),\n",
    "    StructField('fatality_rate', DoubleType(), nullable = True),\n",
    "    StructField('last_updated', IntegerType(), nullable = True),\n",
    "    StructField('recovered', IntegerType(), nullable = True),\n",
    "    StructField('recovered_diff', IntegerType(), nullable = True),\n",
    "    StructField('cities', ArrayType(StringType()), nullable = True),\n",
    "    StructField('iso', DoubleType(), nullable = True),\n",
    "    StructField('lat', DoubleType(), nullable = True),\n",
    "    StructField('long', StringType(), nullable = True),\n",
    "    StructField('name', StringType(), nullable = True),\n",
    "    StructField('province', StringType(), nullable = True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b65010f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = (spark\n",
    "        .read\n",
    "        .schema(raw_schema)\n",
    "        .json(raw_aws_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a4afb17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# This will be used for full or incremental loading\n",
    "\n",
    "def is_curated_exists(curated_aws_path: str) -> bool:\n",
    "    try:\n",
    "        target_df = spark.read.json(curated_aws_path)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "    \n",
    "print(is_curated_exists(curated_aws_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1e64931e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-01\n"
     ]
    }
   ],
   "source": [
    "# Set date for incremental loading\n",
    "\n",
    "if is_curated_exists(curated_aws_path) == False:\n",
    "    date = '2020-01-01'\n",
    "\n",
    "    # Create table if not exists\n",
    "    empty_target_df = spark.createDataFrame([], schema=target_schema)\n",
    "    empty_target_df.write.mode('overwrite').save(curated_aws_path)\n",
    "\n",
    "else:\n",
    "    df = spark.read.json(curated_aws_path)\n",
    "    date = df.select('date').agg(max('date')).collect()[0][0]\n",
    "\n",
    "print(date)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1d0951f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+---------+--------------+----+------+-----------+-------------+------------+---------+--------------+------+---+---+----+----+--------+\n",
      "|active|active_diff|confirmed|confirmed_diff|date|deaths|deaths_diff|fatality_rate|last_updated|recovered|recovered_diff|cities|iso|lat|long|name|province|\n",
      "+------+-----------+---------+--------------+----+------+-----------+-------------+------------+---------+--------------+------+---+---+----+----+--------+\n",
      "+------+-----------+---------+--------------+----+------+-----------+-------------+------------+---------+--------------+------+---+---+----+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empty_target_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1820fd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattening raw data. Region --> cities, iso, lat, long, name, province\n",
    "\n",
    "df_flattened = raw_df                                               \\\n",
    "                .select(\n",
    "                col('active').alias('active'),\n",
    "                col('active_diff').alias('active_diff'),\n",
    "                col('confirmed').alias('confirmed'),\n",
    "                col('confirmed_diff').alias('confirmed_diff'),\n",
    "                col('date').alias('date'),\n",
    "                col('deaths').alias('deaths'),\n",
    "                col('deaths_diff').alias('deaths_diff'),\n",
    "                col('fatality_rate').alias('fatality_rate'),\n",
    "                # col('last_update').alias('last_update'),\n",
    "                col('recovered').alias('recovered'),\n",
    "                col('recovered_diff').alias('recovered_diff'),\n",
    "                col('region.cities').alias('cities'), \n",
    "                col('region.iso').alias('iso').cast('double'),              # Cast to correct type\n",
    "                col('region.lat').alias('lat').cast('double'), \n",
    "                col('region.long').alias('long'), \n",
    "                col('region.name').alias('name'), \n",
    "                col('region.province').alias('province')\n",
    "                )                                                   \\\n",
    "                .filter(col('date')>date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2bbe2269",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'mergeInto'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[43mdf_flattened\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmergeInto\u001b[49m(empty_target_df, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[1;32m      3\u001b[0m \u001b[38;5;241m.\u001b[39mwhenMatched()\u001b[38;5;241m.\u001b[39mupdate({ \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m: df_flattened\u001b[38;5;241m.\u001b[39mdate}) \\\n\u001b[1;32m      4\u001b[0m \u001b[38;5;241m.\u001b[39mwhenNotMatched()\u001b[38;5;241m.\u001b[39minsertAll() \\\n\u001b[1;32m      5\u001b[0m \u001b[38;5;241m.\u001b[39mmerge()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:3123\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3090\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[1;32m   3091\u001b[0m \n\u001b[1;32m   3092\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3120\u001b[0m \u001b[38;5;124;03m+---+\u001b[39;00m\n\u001b[1;32m   3121\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m-> 3123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   3124\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name)\n\u001b[1;32m   3125\u001b[0m     )\n\u001b[1;32m   3126\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mapply(name)\n\u001b[1;32m   3127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'mergeInto'"
     ]
    }
   ],
   "source": [
    "df_flattened \\\n",
    ".mergeInto(empty_target_df, 'date') \\\n",
    ".whenMatched().update({ 'date': df_flattened.date}) \\\n",
    ".whenNotMatched().insertAll() \\\n",
    ".merge()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
